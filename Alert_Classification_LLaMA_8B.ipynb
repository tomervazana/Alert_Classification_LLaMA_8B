{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cybereason AI Engineer Home Assignment"
      ],
      "metadata": {
        "collapsed": false,
        "id": "-dUI9wSanbGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background"
      ],
      "metadata": {
        "collapsed": false,
        "id": "r7gi0eGlnbGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you will receive a dataset of alerts of different vendors and types. Our task is to classify those alerts with MITRE ATT&CK techniques or sub-techniques (for more information about MITRE ATT&CK, please visit https://attack.mitre.org/). <br/>\n",
        "You don't need to understand all MITRE ATT&CK TTP's, but you need to understand the concept of MITRE ATT&CK and how to use it to classify alerts. <br/>\n",
        "We will focus on a few MITRE ATT&CK techniques and sub-techniques."
      ],
      "metadata": {
        "collapsed": false,
        "id": "9IqSSjcHnbGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Task - Classify new coming alerts"
      ],
      "metadata": {
        "collapsed": false,
        "id": "2qPSgP0gnbGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You received 2 datasets: train and test your goal is to build a model based on the train data ( a data with alerts and matching labels) and classify the test data. <br/>\n",
        "Instructions:\n",
        "- You have access to LLaMA-8B model - please LLaMA-8B ONLY [You are NOT allowed to use other models \\ embedding models]\n",
        "- For each single alert generation, you can call the LLM model ONLY ONE TIME.  Also, limit yourself to MAX 8K input tokens.\n",
        "- You are not allowed to use any other open-source model AS IS. If you are choosing to use open-source model for some stage (not mandatory) you have to fine-tune or customize it. The reason for that is to make the task a bit harder :)\n",
        "- You have an API function named `llm` that call the LLM throw an API call - it's limited and should be used for this task only!\n",
        "- Please do not share the API key or use it to other purposes besides this task.\n",
        "- please install the `requrements.txt` file before starting the task."
      ],
      "metadata": {
        "collapsed": false,
        "id": "e4OXWix8nbGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Solution:\n",
        "My final solution below have these key elements on it:\n",
        "* Few-Shot Prompting with Structured Examples (19 shot) on LLaMA-8B,\n",
        "* Adjusting the [model params](https://console.groq.com/docs/text-chat)\n",
        "* Text cleaning - simple cleaning (could get better)\n",
        "* Replacing similar examples with different ones (without embedding model, but menually reading and removing, due to the instruction limitations)\n",
        "* DS Undersampling - To balance the DS\n",
        "* LabelEncoder - encoded the alert code before classification and then decoded them back after prediction\n",
        "* Define the model behaviour with a \"system\" message\n",
        "* JSON Format - The model handles one alert at a time, and return Json, then validate the stracture and return python class with the JSON attributes, it inherit from pydantic-BaseModel the JSON stracture is: {alert: alert text, uid: uid provided with it, code: The label which was encoded by the LabelEncoder, name: alert name, confidence_score: the confidence of the prediction}\n",
        "\n",
        "Measurements:\n",
        "\n",
        "As mentioned above, I took 19 samples to the prompt.\n",
        "\n",
        "I used the other 81 samples to validate the model performance.\n",
        "\n",
        "```\n",
        "Number of correct predictions: 79\n",
        "Number of incorrect predictions: 0\n",
        "Confusion Matrix:\n",
        "[[24  0  0  0  0  0]\n",
        " [ 0 26  0  0  0  0]\n",
        " [ 0  0 20  0  0  0]\n",
        " [ 0  0  0  6  0  0]\n",
        " [ 0  0  0  0  1  0]\n",
        " [ 0  0  0  0  0  2]]\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        24\n",
        "           2       1.00      1.00      1.00        26\n",
        "           4       1.00      1.00      1.00        20\n",
        "           5       1.00      1.00      1.00         6\n",
        "           6       1.00      1.00      1.00         1\n",
        "           7       1.00      1.00      1.00         2\n",
        "\n",
        "    accuracy                           1.00        79\n",
        "   macro avg       1.00      1.00      1.00        79\n",
        "weighted avg       1.00      1.00      1.00        79\n",
        "\n",
        "Micro-Averaged Precision: 1.0\n",
        "Micro-Averaged Recall: 1.0\n",
        "Micro-Averaged F1 Score: 1.0\n",
        "```\n"
      ],
      "metadata": {
        "id": "8IIGx-wsXXKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IkKYBheNpBV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d17b66f-aaa4-4855-d97a-468e630d92b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r '/content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt'\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F6bdom4Boz-L",
        "outputId": "5e47b9b0-4f75-432f-f397-349e9f254112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq (from -r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1))\n",
            "  Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter (from -r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2))\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting python-dotenv (from -r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 3))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (6.5.5)\n",
            "Collecting qtconsole (from jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2))\n",
            "  Downloading qtconsole-5.5.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (7.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 1)) (2.20.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (3.6.7)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (3.0.11)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (5.10.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.1.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2))\n",
            "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (4.19.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (0.19.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (2.22)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter->-r /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/requirements.txt (line 2)) (1.8.0)\n",
            "Installing collected packages: qtpy, python-dotenv, jedi, h11, httpcore, httpx, qtconsole, groq, jupyter\n",
            "Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jedi-0.19.1 jupyter-1.0.0 python-dotenv-1.0.1 qtconsole-5.5.2 qtpy-2.4.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Client\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel # To validate the JSON class stracture\n",
        "import json # for json.dumps on MitreAttack class\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score # To measure performance\n",
        "import pandas as pd # For data operations\n",
        "from sklearn.preprocessing import LabelEncoder # Encode the code into numeric label from 0-(n-1) where n is number of classes\n",
        "import re # for text cleaning\n",
        "\n",
        "# class to represent and validate the JSON\n",
        "class MitreAttack(BaseModel):\n",
        "    alert: str\n",
        "    uid: str\n",
        "    code: int\n",
        "    name: str\n",
        "    confidence_score: float\n",
        "\n",
        "# class representing the llm model\n",
        "class LlamaSmallClassifier():\n",
        "  def __init__(self, env_path):\n",
        "    # load .env file\n",
        "    load_dotenv(dotenv_path=env_path)\n",
        "    # initialize Groq API client\n",
        "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    self.client = Client(api_key=groq_api_key)\n",
        "\n",
        "  def classify(self, prompt, uid, min_label, max_label):\n",
        "    \"\"\"Query the Groq API with a prompt and return JSON response,\n",
        "    then validate the stracture and return python class - MitreAttack with\n",
        "    the JSON attributes\n",
        "    min_label - the minimum label value\n",
        "    max_label - the maximum label value\n",
        "    prompt - the prompt to query the model\n",
        "    uid - the uid provided with the alert\n",
        "    JSON Schema:\n",
        "    {\n",
        "      alert: alert text,\n",
        "      uid: uid provided with it,\n",
        "      code: The label which was encoded by the LabelEncoder,\n",
        "      name: alert name,\n",
        "      confidence_score: the confidence of the prediction\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    json_schema_string = (\n",
        "    \"{\\\"alert\\\":\\\"string (alert message Do not return any additional text, explanations, or comments)\\\",\"\n",
        "    \"\\\"uid\\\":\\\"string (uid provided by the user with the alert)\\\",\"\n",
        "    \"\\\"code\\\":\\\"number (MITRE ATT&CK technique or sub-technique encoded label Do not return any additional text, explanations, or comments)\\\",\"\n",
        "    \"\\\"name\\\":\\\"string (MITRE ATT&CK technique or sub-technique name Do not return any additional text, explanations, or comments)\\\",\"\n",
        "    \"\\\"confidence_score\\\":\\\"float (number (0-1)) Do not return any additional text, explanations, or comments\\\"}\"\n",
        ")\n",
        "\n",
        "    possible_classes = f'({min_label}-{max_label})'\n",
        "    # Find the index of \"code\\\": \\\"number \"\n",
        "    label_index = json_schema_string.find(\"code\\\": \\\"number \")\n",
        "    # Find the index right after \"code\\\": \\\"number \"\n",
        "    insert_position = label_index + len(\"code\\\": \\\"number \")\n",
        "    # insert the possible classes to deny hallucinations\n",
        "    json_schema_string = json_schema_string[:insert_position] + \" \" + possible_classes + json_schema_string[insert_position:]\n",
        "    system_message = (\n",
        "    f\"You are a text classification model. \"\n",
        "    f\"Your task is to classify the given alert message and respond in JSON according to the schema below. \"\n",
        "    f\"Do not return any additional text, explanations, or comments. \"\n",
        "    f\"The JSON schema should include: {json_schema_string}. \"\n",
        "    f\"The JSON should have the following structure: \"\n",
        "    f\"{json.dumps(MitreAttack.model_json_schema(), indent=2)}. \"\n",
        "    f\"Do not return any additional text, explanations, or comments.\"\n",
        "    )\n",
        "    response = self.client.chat.completions.create(\n",
        "      model=os.getenv(\"GROQ_MODEL_NAME\"),\n",
        "      messages=[\n",
        "          {\n",
        "            \"content\": system_message, # Define the model behavior\n",
        "            \"role\": \"system\",\n",
        "            \"name\": uid # The client name assinged as the alert uid\n",
        "          },\n",
        "          {\n",
        "            \"content\": prompt, # Assign the (few shot) prompt of one alert\n",
        "            \"role\": \"user\",\n",
        "            \"name\": uid # The client name assinged as the alert uid\n",
        "          }\n",
        "      ],\n",
        "      max_tokens=512,\n",
        "      temperature=0,\n",
        "      top_p=0.7,\n",
        "      stream=False,\n",
        "      stop=None,\n",
        "      response_format={\"type\": \"json_object\"}\n",
        "      )\n",
        "    return MitreAttack.model_validate_json(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "  def print_classification_report(self, y, y_pred):\n",
        "    \"\"\"Print the confusion matrix, classification report and micro-averaged reports\n",
        "    for the given labels and predictions\n",
        "    y - The ground truth labels\n",
        "    y_pred - The predicted labels\"\"\"\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Calculate classification report\n",
        "    class_report = classification_report(y, y_pred, target_names=None)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(class_report)\n",
        "\n",
        "    # Calculate micro-averaged precision\n",
        "    micro_precision = precision_score(y, y_pred, average='micro')\n",
        "    print(f\"Micro-Averaged Precision: {micro_precision}\")\n",
        "\n",
        "    # Calculate micro-averaged recall\n",
        "    micro_recall = recall_score(y, y_pred, average='micro')\n",
        "    print(f\"Micro-Averaged Recall: {micro_recall}\")\n",
        "\n",
        "    # Calculate micro-averaged F1 score\n",
        "    micro_f1 = f1_score(y, y_pred, average='micro')\n",
        "    print(f\"Micro-Averaged F1 Score: {micro_f1}\")\n",
        "\n",
        "\n",
        "\n",
        "# class representing the prompt\n",
        "class Prompt():\n",
        "  def __init__(self, prompt=\"\"):\n",
        "    self.prompt = prompt\n",
        "\n",
        "  # Using few shot prompting\n",
        "  def design_json_prompt(self, learning_alerts, user_alerts, min_label, max_label):\n",
        "    \"\"\" Returns a prompt to query llama-3-8b for mitre attack alert classification in JSON format \"\"\"\n",
        "    user_alerts_str = \"\"\n",
        "    user_alerts_str += f\"Alert message: {user_alerts['alert']}, uid: {user_alerts['uid']}\\n\"\n",
        "    learning_alerts_str = \"\"\n",
        "    for index, item in learning_alerts.iterrows():\n",
        "      learning_alerts_str += f\"Alert code: {item['code']}, Alert name: {item['name']}, Alert message: {item['alert']}\\n\"\n",
        "    json_schema_string = \"{\\\"alert\\\":\\\"string (alert message Do not return any additional text, explanations, or comments)\\\", \\\"uid\\\":\\\"string (uid provided by the user with the alert)\\\", \\\"code\\\": \\\"number  (MITRE ATT&CK technique or sub-technique encoded label Do not return any additional text, explanations, or comments)\\\", \\\"name\\\": \\\"string (MITRE ATT&CK technique or sub-technique name Do not return any additional text, explanations, or comments)\\\", \\\"confidence_score\\\": \\\"float (number (0-1)) Do not return any additional text, explanations, or comments\\\"}\"\n",
        "\n",
        "    possible_classes = f'({min_label}-{max_label})'\n",
        "    # Find the index of the word \"label\"\n",
        "    label_index = json_schema_string.find(\"code\\\": \\\"number \")\n",
        "    # Find the index right after the word \"label\"\n",
        "    insert_position = label_index + len(\"code\\\": \\\"number \")\n",
        "    json_schema_string = json_schema_string[:insert_position] + \" \" + possible_classes + json_schema_string[insert_position:]\n",
        "    prompt = f\"\"\"\\\n",
        "Here are the learning alerts with their codes, names, and messages:\n",
        "\n",
        "{learning_alerts_str}\n",
        "Here are the new alert messages to classify:\n",
        "{user_alerts_str}\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. For each alert message in the new set, analyze its content to determine the appropriate MITRE ATT&CK technique or sub-technique based on the learning alerts.\n",
        "2. Use the patterns and classifications from the learning alerts to match each new alert message to the correct alert code.\n",
        "3. You must return json of the classification label only. No additional text is allowed.\n",
        "Present the results in the following format only, no other text:\n",
        "<{json_schema_string}>\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "# Methods to handle the data operations\n",
        "\n",
        "def replace_alerts(sampled_alerts, remaining_alerts, new_sampled_alert, new_remaining_alert):\n",
        "  \"\"\"Replace alerts with different ones in dataframes\n",
        "  move new_sampled_alert from remaining_alerts to sampled_alerts\n",
        "  move new_remaining_alert from sampled_alerts to remaining_alerts\n",
        "  This is based on Text and not on index\"\"\"\n",
        "\n",
        "  # Identify the indices\n",
        "  remaining_alerts_index_1 = remaining_alerts.loc[remaining_alerts['alert'] == new_sampled_alert].index[0]\n",
        "  sampled_alerts_index_1 = sampled_alerts.loc[sampled_alerts['alert'] == new_remaining_alert].index[0]\n",
        "\n",
        "  # Extract the rows to be moved\n",
        "  remaining_alerts_row_1 = remaining_alerts.loc[remaining_alerts_index_1]\n",
        "  sampled_alerts_row_1 = sampled_alerts.loc[sampled_alerts_index_1]\n",
        "\n",
        "  # Remove the rows from the original DataFrames\n",
        "  remaining_alerts = remaining_alerts.drop([remaining_alerts_index_1, remaining_alerts_index_2])\n",
        "  sampled_alerts = sampled_alerts.drop(sampled_alerts_index_1)\n",
        "\n",
        "  # Add the rows to the other DataFrames using pd.concat\n",
        "  sampled_alerts = pd.concat([sampled_alerts, pd.DataFrame([remaining_alerts_row_1])], ignore_index=True)\n",
        "  remaining_alerts = pd.concat([remaining_alerts, pd.DataFrame([sampled_alerts_row_1])], ignore_index=True)\n",
        "\n",
        "  return sampled_alerts, remaining_alerts\n",
        "\n",
        "\n",
        "def move_alert(sampled_alerts, remaining_alerts, new_sampled_alert):\n",
        "  \"\"\"move new_sampled_alert from remaining_alerts to sampled_alerts\n",
        "  This is based on Text and not on index\"\"\"\n",
        "\n",
        "  # Identify the indices\n",
        "  remaining_alerts_index_2 = remaining_alerts.loc[remaining_alerts['alert'] == new_sampled_alert].index[0]\n",
        "\n",
        "  # Extract the rows to be moved\n",
        "  remaining_alerts_row_2 = remaining_alerts.loc[remaining_alerts_index_2]\n",
        "\n",
        "  # Remove the rows from the original DataFrames\n",
        "  remaining_alerts = remaining_alerts.drop([remaining_alerts_index_2])\n",
        "\n",
        "  # Add the rows to the other DataFrames using pd.concat\n",
        "  sampled_alerts = pd.concat([sampled_alerts, pd.DataFrame([remaining_alerts_row_2])], ignore_index=True)\n",
        "\n",
        "  return sampled_alerts, remaining_alerts\n",
        "\n",
        "\n",
        "# Does not used in main, but been used in the development process, HAVE not been carefully tested\n",
        "def split_alerts(df):\n",
        "  \"\"\" split the alerts to 2 of each class randomly, not cosidering similarity\n",
        "  this method been used for the first data split and could been more efficient\n",
        "  if it would consider representative of the entire feature distribution of a class \"\"\"\n",
        "  # Group by 'code' and sample 2 alerts for each code, if possible\n",
        "  def sample_alerts(group):\n",
        "      return group.sample(2, random_state=42) if len(group) >= 2 else group\n",
        "\n",
        "  sampled_alerts = df.groupby('code').apply(sample_alerts).reset_index(drop=True)\n",
        "\n",
        "  # Identify the alerts that were not selected\n",
        "  remaining_alerts = df.loc[~df.index.isin(sampled_alerts.index)]\n",
        "\n",
        "  # Display the sampled alerts and the remaining random alerts\n",
        "  print(\"Sampled Alerts:\")\n",
        "  # Display the sampled alerts\n",
        "  for i in range(len(sampled_alerts)):\n",
        "      print(f\"Alert code: {sampled_alerts['code'][i]}, Alert name:{sampled_alerts['name'][i]}, Alert message: {sampled_alerts['alert'][i]}\")\n",
        "\n",
        "  print(\"\\nRandom Remaining Alerts:\")\n",
        "  # Display the remaining random alerts\n",
        "  for i in range(len(random_remaining_alerts)):\n",
        "      print(f\"Alert code: {random_remaining_alerts['code'][i]}, Alert name:{random_remaining_alerts['name'][i]}, Alert message: {random_remaining_alerts['alert'][i]}\")\n",
        "\n",
        "  return sampled_alerts, remaining_alerts\n",
        "\n",
        "\n",
        "# This method assumes that uid is a unique identifier of an alert\n",
        "def get_train_val_alerts(prompt_df_path, data_path):\n",
        "  \"\"\"get the TRAIN ALERTS FOR THE PROMPT EXAMLES\n",
        "  and other data, it validates that df_train and df_val\n",
        "  dont have the same values based on the uid's\n",
        "  prompt_df_path = path to the prompt data df\n",
        "  data_path = path to the original train data provided\"\"\"\n",
        "\n",
        "  df_train = pd.read_csv(prompt_df_path)\n",
        "  data = pd.read_csv(data_path)\n",
        "\n",
        "  # Finding common uids\n",
        "  common_uids = df_train['uid']\n",
        "\n",
        "  # Dropping the rows from `data` with common uids\n",
        "  df_val = data[~data['uid'].isin(common_uids)]\n",
        "\n",
        "  return df_train, df_val\n",
        "\n",
        "# Text Preprocessing function, simple and could do more operations such as:\n",
        "# remove an entire email adress for example\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters and numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# call this method on the train_data if a mistake been made on a prediction\n",
        "def move_incorrect_prediction_to_train(df_train, df_val, correct_predictions):\n",
        "  \"\"\"move incorrect prediction to df_train remove them from df_val\n",
        "  correct_prediction is bool list with True where the predictions were correct\"\"\"\n",
        "  # Get the indices of the wrong predictions\n",
        "  false_indices = [i for i, correct in enumerate(correct_predictions) if not correct]\n",
        "\n",
        "  # Select rows from df_val where correct_predictions is False\n",
        "  rows_to_move = df_val.iloc[false_indices]\n",
        "\n",
        "  # Remove these rows from df_val\n",
        "  df_val = df_val.drop(index=false_indices).reset_index(drop=True)\n",
        "\n",
        "  # Append these rows to df_train using pd.concat\n",
        "  df_train = pd.concat([df_train, rows_to_move]).reset_index(drop=True)\n",
        "\n",
        "  return df_train, df_val\n"
      ],
      "metadata": {
        "id": "t2ACsIGbpbvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  # Load the data\n",
        "  data = pd.read_csv('/content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/train.csv')\n",
        "\n",
        "  # Load the data which was prepared for the prompt\n",
        "  df_train, df_val = get_train_val_alerts(\n",
        "      '/content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/prompt_train.csv',\n",
        "      '/content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/train.csv'\n",
        "      )\n",
        "\n",
        "  # Clean the data\n",
        "  data['alert'] = data['alert'].apply(preprocess_text)\n",
        "  df_train['alert'] = df_train['alert'].apply(preprocess_text)\n",
        "  df_val['alert'] = df_val['alert'].apply(preprocess_text)\n",
        "\n",
        "  # Label Encoding\n",
        "  label_encoder = LabelEncoder()\n",
        "  data['code'] = label_encoder.fit_transform(data['code'])\n",
        "  df_train['code'] = label_encoder.transform(df_train['code'])\n",
        "  df_val['code'] = label_encoder.transform(df_val['code'])\n",
        "\n",
        "  min_label = 0\n",
        "  max_label = len(label_encoder.classes_)-1\n",
        "\n",
        "  # Initialize the LlamaSmallClassifier\n",
        "  llama_small_classifier = LlamaSmallClassifier(\"/content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/.env\")\n",
        "\n",
        "  # Initialize the Prompt class\n",
        "  prompt = Prompt()\n",
        "\n",
        "  tqdm.pandas()\n",
        "\n",
        "  mitre_attack_arr = []\n",
        "  y_preds = []\n",
        "  y = []\n",
        "\n",
        "  # Function to process each row\n",
        "  def process_row(row, prompt, llama_small_classifier, df_train, min_label, max_label):\n",
        "      my_prompt = prompt.design_json_prompt(learning_alerts=df_train, user_alerts=row, min_label=min_label, max_label=max_label)\n",
        "      mitre_attack = llama_small_classifier.classify(my_prompt, row['uid'], min_label, max_label)\n",
        "      y_preds.append(mitre_attack.code)\n",
        "      y.append(row['code'])\n",
        "      return mitre_attack\n",
        "\n",
        "  # Apply the function to each row with a progress bar, using a lambda to pass additional arguments\n",
        "  mitre_attack_arr = df_val.progress_apply(lambda row: process_row(row, prompt, llama_small_classifier, df_train, min_label, max_label), axis=1)\n",
        "\n",
        "  # Ensure the results are properly added to mitre_attack_arr\n",
        "  mitre_attack_arr = list(mitre_attack_arr)\n",
        "\n",
        "  # Convert to numpy arrays\n",
        "  actuals = np.array(y)\n",
        "  predictions = np.array(y_preds) # we can save space by using mitre_attack_arr[i]['code'], this is more readable in my opinion\n",
        "\n",
        "  # Calculate correct predictions\n",
        "  correct_predictions = (actuals == predictions)\n",
        "  num_correct = np.sum(correct_predictions)\n",
        "  num_incorrect = len(actuals) - num_correct\n",
        "\n",
        "  print(f\"Number of correct predictions: {num_correct}\")\n",
        "  print(f\"Number of incorrect predictions: {num_incorrect}\")\n",
        "\n",
        "  # if we had mistakes, can also remove similars from df_train if req.\n",
        "  # df_train, df_val = move_incorrect_prediction_to_train(df_train, df_val, correct_predictions)\n",
        "  # inverse_transform WITH LABEL ENCODER BEFORE SAVING THE NEW SAMPLES!!\n",
        "\n",
        "\n",
        "  llama_small_classifier.print_classification_report(actuals, predictions)\n",
        "\n",
        "  mitre_attack_arr = []\n",
        "\n",
        "  # Function to process each row\n",
        "  def process_row(row, prompt, llama_small_classifier, df_train, min_label, max_label):\n",
        "      my_prompt = prompt.design_json_prompt(learning_alerts=df_train, user_alerts=row, min_label=min_label, max_label=max_label)\n",
        "      mitre_attack = llama_small_classifier.classify(my_prompt, row['uid'], min_label, max_label)\n",
        "      return mitre_attack\n",
        "\n",
        "  # Now lets predict test results\n",
        "\n",
        "  df_test = pd.read_csv('/content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/test.csv')\n",
        "\n",
        "  # Apply the function to each row with a progress bar, using a lambda to pass additional arguments\n",
        "  mitre_attack_arr = df_test.progress_apply(lambda row: process_row(row, prompt, llama_small_classifier, df_train, min_label, max_label), axis=1)\n",
        "\n",
        "  # Ensure the results are properly added to mitre_attack_arr\n",
        "  mitre_attack_arr = list(mitre_attack_arr)\n",
        "\n",
        "  # Convert encoded labels back to original codes\n",
        "  for attack in mitre_attack_arr:\n",
        "      attack.code = label_encoder.inverse_transform([attack.code])[0]\n",
        "\n",
        "  res_data = [(attack.uid, attack.code) for attack in mitre_attack_arr]\n",
        "  # Create a DataFrame\n",
        "  df = pd.DataFrame(res_data, columns=['uid', 'code'])\n",
        "\n",
        "  # Specify the output file path\n",
        "  output_file_path = '/content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/Test Predictions.csv'\n",
        "\n",
        "  # Save the DataFrame to a CSV file\n",
        "  df.to_csv(output_file_path, index=False)\n",
        "\n",
        "  print(f\"Data has been saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsu8TdDdgIj3",
        "outputId": "9cb1ed07-c169-40b4-9b45-102352f45c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [03:18<00:00,  2.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of correct predictions: 79\n",
            "Number of incorrect predictions: 0\n",
            "Confusion Matrix:\n",
            "[[24  0  0  0  0  0]\n",
            " [ 0 26  0  0  0  0]\n",
            " [ 0  0 20  0  0  0]\n",
            " [ 0  0  0  6  0  0]\n",
            " [ 0  0  0  0  1  0]\n",
            " [ 0  0  0  0  0  2]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        24\n",
            "           2       1.00      1.00      1.00        26\n",
            "           4       1.00      1.00      1.00        20\n",
            "           5       1.00      1.00      1.00         6\n",
            "           6       1.00      1.00      1.00         1\n",
            "           7       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00        79\n",
            "   macro avg       1.00      1.00      1.00        79\n",
            "weighted avg       1.00      1.00      1.00        79\n",
            "\n",
            "Micro-Averaged Precision: 1.0\n",
            "Micro-Averaged Recall: 1.0\n",
            "Micro-Averaged F1 Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [02:27<00:00,  3.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to /content/drive/MyDrive/AI Engineer Home Assignment-20240709T133517Z-001/AI Engineer Home Assignment/Test Predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "- Evaluate your results - How many correct answers you have? How many incorrect?\n",
        "- Do you have any suggestion to make the results more accurate? (Do not implement)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "AJvJZofdnbGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Evaluate your results - How many correct answers you have? How many incorrect?\n",
        "```\n",
        "Few-Shot Prompting with Structured Examples:\n",
        "Number of correct predictions: 79\n",
        "Number of incorrect predictions: 0\n",
        "```\n",
        "\n",
        "*   Do you have any suggestion to make the results more accurate? (Do not implement)\n",
        "\n",
        "\n",
        "1. Try more prompt Eng techniques\n",
        "(Zero-shot Prompting\n",
        "Chain-of-Thought Prompting\n",
        "Self-Consistency\n",
        "Generate Knowledge Prompting\n",
        "Prompt Chaining\n",
        "Tree of Thoughts\n",
        "Retrieval Augmented Generation\n",
        "Automatic Reasoning and Tool-use\n",
        "Automatic Prompt Engineer\n",
        "Active-Prompt\n",
        "Directional Stimulus Prompting\n",
        "Program-Aided Language Models\n",
        "ReAct\n",
        "Reflexion\n",
        "Graph Prompting)\n",
        "2. Try different data manipulations such as paraphrasing, synonym replacement, back-translation, Normalize text in different ways\n",
        "3. Try create synthetic data and balance the DS\n",
        "4. Try different model params\n",
        "5. Supervised Fine-Tuning / RAG / PEFT\n",
        "6. Try using the most accurate model for Text classification\n",
        "7. Analyze misclassified alerts to understand common failure modes and iteratively improve the model\n",
        "8. Ensure data diversity between specific class possible examples (instead of random sampling, can be done with siamese model and cosine similarity)\n",
        "9. Add all the possible classes to the prompt (of mitre att&ck in general)\n",
        "10. check confidence score in the json for more samples and move samples with low confidence score to the prompt (but make sure to not overfit the model)"
      ],
      "metadata": {
        "id": "6z4O1CGabQis"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4cSBThc7tvkw",
        "lks7FFCjHeKX",
        "M73_z6E7Sr93",
        "Ri7KUseOUahU",
        "xiWrGKKOfGfL",
        "jrQ-Vj9mvhxP",
        "CTjv0WR8Hj55",
        "2OELpJsyb3L-"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}